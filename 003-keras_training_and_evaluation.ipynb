{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow2教程-使用keras训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本指南包含了TensorFlow 2.0中在以下两种情况下的训练，评估和预测（推理）模型：\n",
    "\n",
    "- 使用内置的训练和评估API（例如model.fit()，model.evaluate()，model.predict()）。\n",
    "- 使用eager execution 和GradientTape对象从头开始编写自定义循环。\n",
    "\n",
    "无论是使用内置循环还是编写自己的循环，模型和评估训练在每种Keras模型中严格按照相同的方式工作，无论是Sequential 模型, 函数式 API, 还是模型子类化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q pydot\n",
    "# !apt-get install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 一般的模型构造、训练、测试流程\n",
    "使用内置的训练和评估API对模型进行训练和验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 模型构造\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h1 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h1)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "端到端的模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') /255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') /255\n",
    "\n",
    "# 保证还是float 32？ 否则后面会出现：TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type uint8 of argument 'x'.\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "# 取验证数据\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=3,\n",
    "         validation_data=(x_val, y_val))\n",
    "print('history:')\n",
    "print(history.history)\n",
    "\n",
    "result = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('evaluate:')\n",
    "print(result)\n",
    "pred = model.predict(x_test[:2])\n",
    "print('predict:')\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 自定义指标和损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 配置网络\n",
    "在对模型训练之前，我们需要指定损失函数，优化器以及可选的一些要监控的指标。我们将这些配置参数作为compile()方法的参数传递给模型，对模型进行配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow2提供许多内置的优化器，损失和指标\n",
    "常见的内置参数如下：\n",
    "\n",
    "- 优化器： - SGD()（有或没有动量） - RMSprop() - Adam() -等等。\n",
    "\n",
    "- 损失： - MeanSquaredError() - KLDivergence() - CosineSimilarity() -等等。\n",
    "\n",
    "- 指标： - AUC() - Precision() - Recall() -等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 自定义损失\n",
    "用Keras提供两种方式来提供自定义损失。\n",
    "- 一、例创建一个接受输入y_true和的函数y_pred。\n",
    "- 二、构建一个继承keras.losser.Loss的子类\n",
    "下面示例显示了一个损失函数，该函数计算实际数据与预测之间的平均距离："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='digits')\n",
    "    x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "model = get_uncompiled_model()\n",
    "\n",
    "def basic_loss_function(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(y_true - y_pred)\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=basic_loss_function)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要试下带参数的损失函数，可以子类化tf.keras.losses.Loss。并子类化以下方法：\n",
    "- \\__init\\__(self)  接收相关参数，初始化loss之类。\n",
    "- call(self, y_true, y_pred)  使用 y_true和y_pred，计算模型损失。\n",
    "\n",
    "下面例子，展示了WeightedCrossEntropy计算二分损失的损失函数，某个类或整个函数的损失可以通过标量修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class WeightBinaryCrossEntropy(keras.losses.Loss):\n",
    "    def __init__(self, pos_weight, weight, from_logits=False,\n",
    "                reduction=keras.losses.Reduction.AUTO,\n",
    "                name='weight_binary_crossentropy'):\n",
    "        \"\"\"\n",
    "        pos_weight: 正类标签权重\n",
    "        weight: 整体损失权重\n",
    "        from_logits: 是否使用logits来计算loss，（或使用probability）\n",
    "        reduction: reduction类型\n",
    "        name: 名字\n",
    "        \"\"\"\n",
    "        super(WeightBinaryCrossEntropy, self).__init__(reduction=reduction,\n",
    "                                                      name=name)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.from_logits = from_logits\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        if not self.from_logits:\n",
    "            x_1 = y_true * self.pos_weight * -tf.math.log(y_pred + 1e-6)\n",
    "            \n",
    "            x_2 = (1-y_true) * -tf.math.log(1-y_pred + 1e-6)\n",
    "            \n",
    "            return tf.add(x_1, x_2) * self.weight\n",
    "        \n",
    "        return tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.pos_weight) *self.weight\n",
    "        \n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=WeightBinaryCrossEntropy(0.5, 2))\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用网络层的方法构建loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs)* 0.1)\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# 添加计算正则化损失的层\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# 训练\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 自定义指标\n",
    "\n",
    "自定义指标只需继承Metric类， 并重写以下函数：\n",
    "\n",
    "- \\__init\\__(self)，初始化。\n",
    "\n",
    "- update_state(self，y_true，y_pred，sample_weight = None)，它使用目标y_true和模型预测y_pred来更新状态变量。\n",
    "\n",
    "- result(self)，它使用状态变量来计算最终结果。\n",
    "\n",
    "- reset_states(self)，重新初始化度量的状态。\n",
    "\n",
    "状态更新和结果计算保持分开（分别在update_state()和result()中），因为在某些情况下，结果计算可能非常昂贵，并且只能定期进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 下面是一个简单的示例，显示如何实现CatgoricalTruePositives指标，该指标计算正确分类为属于给定类的样本数量\n",
    "\n",
    "class CatgoricalTruePostives(keras.metrics.Metric):\n",
    "    def __init__(self, name='binary_true_postives', **kwargs):\n",
    "        super(CatgoricalTruePostives, self).__init__(name=name, **kwargs)\n",
    "        # 会更新的类变量\n",
    "        self.true_postives = self.add_weight(name='tp', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # 获取结果id\n",
    "        y_pred = tf.argmax(y_pred)\n",
    "        # 正确的结果\n",
    "        y_true = tf.equal(tf.cast(y_pred, tf.int32), tf.cast(y_true, tf.int32))\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            # 对正确结果加权重\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            y_true = tf.multiply(sample_weight, y_true)\n",
    "        # 修改正确样本总量\n",
    "        return self.true_postives.assign_add(tf.reduce_sum(y_true))\n",
    "    \n",
    "    def result(self):\n",
    "        # 返回相应tensor\n",
    "        return tf.identity(self.true_postives)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        # 重置为0\n",
    "        self.true_postives.assign(0.)\n",
    "        \n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[CatgoricalTruePostives()])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         batch_size=64, epochs=3)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用自定义层的方式获取相关指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 也可以以定义网络层的方式添加要统计的metric\n",
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # 该层的作用就是添加指标\n",
    "        self.add_metric(keras.backend.std(inputs),\n",
    "                       name='std_of_activation',\n",
    "                       aggregation='mean')\n",
    "        # 直接把输入进行输出\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "# 直接套在对应的网络层中\n",
    "h1 = MetricLoggingLayer()(h1)\n",
    "h1 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h1)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "# 配置并训练网络\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以在构建好模型后直接使用，model.add_loss和model.add_metric添加损失和指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h2 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h2)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# 直接把计算loss或metric用到的输入(h1)带人\n",
    "model.add_metric(keras.backend.std(h1),\n",
    "                       name='std_of_activation',\n",
    "                       aggregation='mean')\n",
    "\n",
    "model.add_loss(tf.reduce_sum(h1)*0.1)\n",
    "\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理使用validation_data传入测试数据，还可以使用validation_split划分验证数据\n",
    "\n",
    "ps:validation_split只能在用numpy数据训练的情况下使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 使用tf.data构造数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到现在我们已经了解了如何使用使用numpy作为输入数据进行训练和验证。下面，我们将介绍如何使用tf.data作为输入数据进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "    h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "    h2 = layers.Dense(64, activation='relu')(h1)\n",
    "    outputs = layers.Dense(10, activation='softmax')(h2)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                 loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "model = get_compiled_model()\n",
    "# 构建dataset实例\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# 打乱\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "# 获得验证数据\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "# model.fit(train_dataset, epochs=3)\n",
    "# steps_per_epoch 每个epoch只训练几步\n",
    "# validation_steps 每次验证，验证几步\n",
    "model.fit(train_dataset, epochs=3, steps_per_epoch=100,\n",
    "         validation_data=val_dataset, validation_steps=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想对该数据集中的特定批次进行训练，则可以传递steps_per_epoch参数，即批训练多少步。同样我们可以使用train_dataset.take(), 来获取每批次中训练的数据，其和steps_per_epoc等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(train_dataset.take(100), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "print('\\n# Evaluate')\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**其他格式输入数据支持**\n",
    "\n",
    "除了numpy数值和TensorFlow Dataset,还可以用Pandas和python迭代器作为数据输入。\n",
    "通常小数据推荐使用numpy， 大数据推荐使用TensorFlow Dataset。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 样本权重和类权重\n",
    "在模型训练时可以，可以人工设定样本权重和类权重。\n",
    "\n",
    "“样本权重”数组是一个数字数组，用于指定批处理中每个样本在计算总损失时应具有多少权重。 它通常用于不平衡的分类问题（这个想法是为了给予很少见的类更多的权重）。 当使用的权重是1和0时，该数组可以用作损失函数的掩码（完全丢弃某些样本对总损失的贡献）。\n",
    "\n",
    "“类权重”dict是同一概念的更具体的实例：它将类索引映射到应该用于属于该类的样本的样本权重。 例如，如果类“0”比数据中的类“1”少两倍，则可以使用class_weight = {0：1.，1：0.5}。\n",
    "\n",
    "添加方法：\n",
    "- 使用Numpy数据时： 通过sample_weight和class_weight参数传递。\n",
    "- 使用Dataset数据时： 通过使数据集返回(input_batch, target_batch, sample_weight_batch)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个Numpy数据中加大第5类的权重的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 增加第5类的权重\n",
    "import numpy as np\n",
    "# 类权重\n",
    "model = get_compiled_model()\n",
    "class_weight = {i:1.0 for i in range(10)}\n",
    "# 第5类的权重为2\n",
    "class_weight[5] = 2.0\n",
    "print(class_weight)\n",
    "model.fit(x_train, y_train,\n",
    "         class_weight=class_weight,\n",
    "         batch_size=64,\n",
    "         epochs=4)\n",
    "# 样本权重\n",
    "model = get_compiled_model()\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "model.fit(x_train, y_train,\n",
    "         sample_weight=sample_weight,\n",
    "         batch_size=64,\n",
    "         epochs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset数据中加大第5类的权重的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# tf.data数据\n",
    "model = get_compiled_model()\n",
    "\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "# 在构造dataset时传入sample_weight\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train,\n",
    "                                                    sample_weight))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 多输入多输出模型\n",
    "有些模型是是多输入或多输出的，此时可以为他们设置不同的权重的loss和metric。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image_input = keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = keras.Input(shape=(None, 10), name='ts_input')\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name='score_output')(x)\n",
    "class_output = layers.Dense(5, activation='softmax', name='class_output')(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])\n",
    "keras.utils.plot_model(model, 'multi_input_output_model.png'\n",
    "                       , show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置不同的loss和metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 可以为模型指定不同的loss和metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# 还可以指定loss的权重\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weight={'score_output': 2., 'class_output': 1.})\n",
    "\n",
    "# 可以把不需要传播的loss置0\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# 可以使用字典的方式设置\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'class_output': keras.losses.CategoricalCrossentropy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为多输入和多输出模型构造numpy数据，并训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# 生成数据\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# 训练\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "# 可以使用字典匹配输入输出数据\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Dataset构造数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'img_input': img_data, 'ts_input': ts_data},\n",
    "     {'score_output': score_targets, 'class_output': class_targets}))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 使用回调\n",
    "Keras中的回调是在训练期间（在epoch开始时，batch结束时，epoch结束时等）与不同时间点调用的对象，可用于实现以下行为：\n",
    "\n",
    "- 在训练期间的不同时间点进行验证（除了内置的按时间段验证）\n",
    "- 定期检查模型是否超过某个精度阈值\n",
    "- 在训练似乎停滞不前时，改变模型的学习率\n",
    "- 在训练似乎停滞不前时，对顶层进行微调\n",
    "- 在训练结束或超出某个性能阈值时发送电子邮件或即时消息通知等等。\n",
    "\n",
    "**可使用的内置回调有**\n",
    "\n",
    "- ModelCheckpoint：定期保存模型。\n",
    "- EarlyStopping：当训练不再改进验证指标时停止培训。\n",
    "- TensorBoard：定期编写可在TensorBoard中显示的模型日志（更多细节见“可视化”）。\n",
    "- CSVLogger：将丢失和指标数据流式传输到CSV文件。\n",
    "- 等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 回调使用\n",
    "下面是几个回调使用的简单例子\n",
    "\n",
    "1）提前终止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # 不再提升的关注指标\n",
    "        monitor='val_loss',\n",
    "        # 不再提升的阈值\n",
    "        min_delta=1e-2,\n",
    "        # 不再提升的轮次\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint模型回调\n",
    "model = get_compiled_model()\n",
    "check_callback = keras.callbacks.ModelCheckpoint(\n",
    "    # 模型路径\n",
    "    filepath='mymodel_{epoch}.h5',\n",
    "    # 是否保存最佳\n",
    "    save_best_only=True,\n",
    "    # 监控指标\n",
    "    monitor='val_loss',\n",
    "    # 进度条类型\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=3,\n",
    "         batch_size=64,\n",
    "         callbacks=[check_callback],\n",
    "         validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）学习率调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 动态调整学习率\n",
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    # 初始学习率\n",
    "    initial_learning_rate,\n",
    "    # 延迟步数\n",
    "    decay_steps=10000,\n",
    "    # 调整百分比\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=3,\n",
    "         batch_size=64,\n",
    "         callbacks=[check_callback],\n",
    "         validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）训练可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 使用tensorboard\n",
    "tensorboard_cbk = keras.callbacks.TensorBoard(log_dir='./full_path_to_your_logs')\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=5,\n",
    "         batch_size=64,\n",
    "         callbacks=[tensorboard_cbk],\n",
    "         validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6.2 创建自己的回调方法\n",
    "构建一个可以记录每个epoch的loss的回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.losses = []\n",
    "    def on_epoch_end(self, batch, logs):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        print('\\nloss:',self.losses[-1])\n",
    "        \n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    LossHistory()\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 自己构造训练和验证循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以自定义循环和验证方法，而不是简单调用fit和evaluate()。\n",
    "\n",
    "### 7.1使用GradientTape构建训练\n",
    "GradientTape可以计算变量梯度，并实现反向传播的功能。我们下面使用GradientTape来构建自定义训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 优化器和损失函数\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# 准备数据\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 自己构造循环\n",
    "for epoch in range(3):\n",
    "    print('epoch: ', epoch)\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # 开一个gradient tape, 计算梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            # 获取loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            # 由loss计算各变量梯度\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            # 使用优化器计算反向传播\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 在自定义训练中设定指标\n",
    "我们可以在自定义训练循环中随时使用内置指标（或编写的自定义指标）。流程如下：\n",
    "\n",
    "- 在循环开始时实例化指标\n",
    "- 每一批数据训练之后，调用metric.update_state()\n",
    "- 需要获得指标时，调用metric.result()\n",
    "- 需要清除指标时，调用metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 训练并验证\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# 实例化指标类.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy() \n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# 准备数据\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "\n",
    "# 自定义训练迭代\n",
    "for epoch in range(3):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "  \n",
    "    # 获取每一批数据。\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        # 计算梯度， 反向传播\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # 更新指标\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "        # 输出\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "\n",
    "    # 每一批次结束，获取一次指标\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    # 重置指标\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # 迭代验证\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        # Update val metrics\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    # 获取验证指标\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 在自定义训练中添加loss\n",
    "\n",
    "我们可以在自定义训练中添加自定义损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "##　添加自己构造的loss, 每次只能看到最新一次训练增加的loss\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "    return inputs\n",
    "  \n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "# 添加正则化损失层\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "logits = model(x_train[:64])\n",
    "print(model.losses)\n",
    "logits = model(x_train[:64])\n",
    "logits = model(x_train[64: 128])\n",
    "logits = model(x_train[128: 192])\n",
    "print(model.losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 将loss添加进求导中\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # 添加额外的损失\n",
    "            loss_value += sum(model.losses)\n",
    "        # 求梯度，反向传播\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # 输出\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}